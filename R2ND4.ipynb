{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "R2ND4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcXH9DIZE-NU"
      },
      "source": [
        "# Reinforcement Learning Assignment: Gravitar\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oTZnMJyGjWk"
      },
      "source": [
        "Recurrent Replay Non-Distributed Deeper Denser DQN (R2ND4) is a Reinforcement Learning Agent that was initially designed as a non-distributed version of R2D2 [1] and was later developed further. R2ND4 uses: \n",
        "* Double Q-Learning [4];\n",
        "* Prioritized Replay Buffer [6] using transition sequences of length 120 [1];\n",
        "* n-step Bellman Rewards and Targets [4];\n",
        "* Invertible Value Function Rescaling (forward and inverse) [7];\n",
        "* A Duelling Network Architecture [3];\n",
        "* A CNN followed by an LSTM for state encodings (with burnin as per R2D2 [1]);\n",
        "* A Novel Deeper Denser Architecture using Skip-Connections, inspired by D2RL's [2] findings;\n",
        "* Gradient Clipping as recommended in [3];\n",
        "* Frame Stacking;\n",
        "* Observations resized to 84x84 and turned to greyscale using OpenCV.\n",
        "\n",
        "For training on a less complex environment or if prioritising faster convergence, the 2nd much wider linear layer from the Value and Advantage networks containing the skip connection to the CNN output can be removed entirely to allow the model to converge to a lower 3,000 rolling mean score after 180k episodes (and reaching 3,500 after 500k episodes).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIUx_Srn6x6O"
      },
      "source": [
        "## References\n",
        "The findings of the following papers were relied upon for the design of R2ND4:\n",
        "* Recurrent Experience Replay in Distributed Reinforcement Learning (R2D2) [1]: https://openreview.net/forum?id=r1lyTjAqYX;\n",
        "* D2RL: Deep Dense Architectures in Reinforcement Learning  [2]: https://arxiv.org/abs/2010.09163;\n",
        "* Dueling Network Architectures for Deep Reinforcement Learning [3]: https://arxiv.org/abs/1511.06581;\n",
        "* Rainbow: Combining Improvements in Deep Reinforcement Learning [4]: https://arxiv.org/abs/1710.02298;\n",
        "* Distributed Prioritized Experience Replay [5]: https://arxiv.org/abs/1803.00933;\n",
        "* Prioritized Experience Replay [6]: https://arxiv.org/abs/1511.05952;\n",
        "* Observe and Look Further: Achieving Consistent Performance on Atari [7]: https://arxiv.org/abs/1805.11593.\n",
        "\n",
        "The codebase below is based upon and borrows from the following sources:\n",
        "* SEED RL: Scalable and Efficient Deep-RL with Accelerated Central Inference [8]: https://github.com/google-research/seed_rl;\n",
        "* Reinforcement Learning Assembly (rela) [9]: https://github.com/facebookresearch/rela;\n",
        "* RL Adventure [10]: https://github.com/higgsfield/RL-Adventure;\n",
        "* OpenAI Baselines [11]: https://github.com/openai/baselines;\n",
        "* Distributed Reinforcement Learning [12]: https://github.com/chagmgang/distributed_reinforcement_learning.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klgBp_bRFqYU"
      },
      "source": [
        "## Imports and TensorBoard + CUDA setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOkHfWc4E5ii",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af7bdb6a-088a-4cc6-e9d2-955ea2ed3b87"
      },
      "source": [
        "import gym\n",
        "from gym import spaces\n",
        "import collections\n",
        "import random\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import cv2\n",
        "import operator\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "# Set the tag for saving and TensorBoard.\n",
        "tag = \"R2ND4\"\n",
        "\n",
        "# Whether to restart training from a checkpoint located at training/{tag}.\n",
        "restart = False\n",
        "if not os.path.exists(\"training\"):\n",
        "    os.makedirs(\"training\")\n",
        "\n",
        "# Setup TensorBoard to write to runs/{tag}.\n",
        "writer = SummaryWriter(\"runs//{}\".format(tag))\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "print(use_cuda)\n",
        "device   = torch.device(\"cuda\" if use_cuda else \"cpu\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHg2KXzwF-UN"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUHdRMoFGCgl"
      },
      "source": [
        "# Adam optimizer hyperparameters.\n",
        "learning_rate                = 0.0001\n",
        "adam_eps                     = 0.001\n",
        "\n",
        "# Sync rate between target and online networks for Double Q-Learning.\n",
        "sync_target_every       = 250\n",
        "\n",
        "# Batch size of length seq_len_with_burn_in sequences of transitions to use for the networks and Replay Buffer.\n",
        "batch_size              = 64\n",
        "\n",
        "# Hyperparameters used by the Prioritized Replay Buffer\n",
        "eta                          = 0.9  # Weighting of max (eta) and mean (1-eta) of TD errors when calculating the priority of a batch post sampling to update priorities within buffer.\n",
        "priority_exponent            = 0.9  # alpha\n",
        "importance_sampling_exponent = 0.6  # beta\n",
        "buffer_limit            = 15000     # Size of buffer.\n",
        "\n",
        "# Epsilon value used in value function scaling and rescaling (taken from R2D2).\n",
        "vfEpsilon   = 0.001\n",
        "\n",
        "# Number of steps used for n-step Bellman rewards and targets.\n",
        "n_steps                 = 5\n",
        "\n",
        "# Discount factor for future rewards.\n",
        "gamma                   = 0.99\n",
        "\n",
        "# Lengths used for storing sequences of experience into the Replay Buffer (See R2D2 for details).\n",
        "seq_len                 = 80 + n_steps\n",
        "l_burnin                = 40\n",
        "seq_len_with_burn_in    = seq_len + l_burnin\n",
        "overlap = int(seq_len / 2) # 40\n",
        "seq_len_with_burn_in_minus_overlap    = seq_len_with_burn_in - overlap\n",
        "\n",
        "# Minimum length of sequence to submit to the buffer (otherwise will be discarded). This is a personal contribution.\n",
        "minimumLen = 20\n",
        "\n",
        "# For handling statistics and videos.\n",
        "video_every             = 100\n",
        "print_every             = 25"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKFJKUxFJhCi"
      },
      "source": [
        "## Gym environment wrappers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXpJVqbMJriC"
      },
      "source": [
        "# Taken from RL Adventure's [10] and OpenAI's Baselines [11] Repos.\n",
        "class EpisodicLifeEnv(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        \"\"\"Make end-of-life == end-of-episode, but only reset on true game over.\n",
        "        Done by DeepMind for the DQN and co. since it helps value estimation.\n",
        "        \"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        self.lives = 0\n",
        "        self.was_real_done  = True\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, reward, done, info = self.env.step(action)\n",
        "        self.was_real_done = done\n",
        "        # check current lives, make loss of life terminal,\n",
        "        # then update lives to handle bonus lives\n",
        "        lives = self.env.unwrapped.ale.lives()\n",
        "        if lives < self.lives and lives > 0:\n",
        "            # for Qbert sometimes we stay in lives == 0 condtion for a few frames\n",
        "            # so its important to keep lives > 0, so that we only reset once\n",
        "            # the environment advertises done.\n",
        "            done = True\n",
        "        self.lives = lives\n",
        "        return obs, reward, done, info\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        \"\"\"Reset only when lives are exhausted.\n",
        "        This way all states are still reachable even though lives are episodic,\n",
        "        and the learner need not know about any of this behind-the-scenes.\n",
        "        \"\"\"\n",
        "        if self.was_real_done:\n",
        "            obs = self.env.reset(**kwargs)\n",
        "        else:\n",
        "            # no-op step to advance from terminal/lost life state\n",
        "            obs, _, _, _ = self.env.step(0)\n",
        "        self.lives = self.env.unwrapped.ale.lives()\n",
        "        return obs\n",
        "\n",
        "# Taken from RL Adventure's [10] and OpenAI's Baselines [11] Repos.\n",
        "class WarpFrame(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        \"\"\"Warp frames to 84x84 as done in the Nature paper and later work.\"\"\"\n",
        "        gym.ObservationWrapper.__init__(self, env)\n",
        "        self.width = 84\n",
        "        self.height = 84\n",
        "        self.observation_space = spaces.Box(low=0, high=255,\n",
        "            shape=(self.height, self.width, 1), dtype=np.uint8)\n",
        "\n",
        "    def observation(self, frame):\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
        "        frame = cv2.resize(frame, (self.width, self.height), interpolation=cv2.INTER_AREA)\n",
        "        return frame[:, :, None]\n",
        "\n",
        "# Edited from RL Adventure's [10] and OpenAI's Baselines [11] Repos.\n",
        "class FrameStack(gym.Wrapper):\n",
        "    def __init__(self, env, k):\n",
        "        \"\"\"Stack k last frames.\n",
        "        Returns lazy array, which is much more memory efficient.\n",
        "        See Also\n",
        "        --------\n",
        "        baselines.common.atari_wrappers.LazyFrames\n",
        "        \"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        self.k = k\n",
        "        self.frames = collections.deque([], maxlen=k)\n",
        "        shp = env.observation_space.shape\n",
        "        self.observation_space = spaces.Box(low=0, high=255, shape=(k * shp[2], shp[0], shp[1]), dtype=np.uint8)\n",
        "\n",
        "    def reset(self):\n",
        "        ob = self.env.reset().transpose(2,0,1)\n",
        "        for _ in range(self.k):\n",
        "            self.frames.append(ob)\n",
        "        return self._get_ob()\n",
        "\n",
        "    def step(self, action):\n",
        "        ob, reward, done, info = self.env.step(action)\n",
        "        self.frames.append(ob.transpose(2,0,1))\n",
        "        return self._get_ob(), reward, done, info\n",
        "\n",
        "    def _get_ob(self):\n",
        "        assert len(self.frames) == self.k\n",
        "        return LazyFrames(list(self.frames))\n",
        "\n",
        "# Taken from RL Adventure's [10] and OpenAI's Baselines [11] Repos.\n",
        "class LazyFrames(object):\n",
        "    def __init__(self, frames):\n",
        "        \"\"\"This object ensures that common frames between the observations are only stored once.\n",
        "        It exists purely to optimize memory usage which can be huge for DQN's 1M frames replay\n",
        "        buffers.\n",
        "        This object should only be converted to numpy array before being passed to the model.\n",
        "        You'd not believe how complex the previous solution was.\"\"\"\n",
        "        self._frames = frames\n",
        "        self._out = None\n",
        "\n",
        "    def _force(self):\n",
        "        if self._out is None:\n",
        "            self._out = np.concatenate(self._frames, axis=0)\n",
        "            self._frames = None\n",
        "        return self._out\n",
        "\n",
        "    def __array__(self, dtype=None):\n",
        "        out = self._force()\n",
        "        if dtype is not None:\n",
        "            out = out.astype(dtype)\n",
        "        return out\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._force())\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self._force()[i]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIIFuvYXKOnB"
      },
      "source": [
        "## Gym environment setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvx5EN_uKeow"
      },
      "source": [
        "# Make environment\n",
        "env = gym.make('Gravitar-v0')\n",
        "\n",
        "# Unlike R2D2's reported results [1], using episodic life (with roll-over LSTM states) resulted in worse performance, so this wrapper is disabled for our model (NOTE: requires tweaking in the acting loop to enable roll-over)\n",
        "# env = EpisodicLifeEnv(env)\n",
        "\n",
        "# Resize frame to 84x84 and make grayscale.\n",
        "env = WarpFrame(env)\n",
        "\n",
        "# Makes env observation into last 4 frames instead.\n",
        "env = FrameStack(env, 4)\n",
        "\n",
        "# Enables video recording.\n",
        "env = gym.wrappers.Monitor(env, \"./video\", video_callable=lambda episode_id: (episode_id%video_every)==0,force=True)\n",
        "\n",
        "num_actions = env.action_space.n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJdkG4lFkZzc"
      },
      "source": [
        "## Setup Reproducible Environment and Action Spaces"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0IlgZuWkgHy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "728c823e-62c5-4377-f411-3c3f0358a67a"
      },
      "source": [
        "seed = 742\n",
        "torch.manual_seed(seed)\n",
        "env.seed(seed)\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "env.action_space.seed(seed)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[742]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2EbGebMLurT"
      },
      "source": [
        "## Prioritized Replay Buffer Prerequisites"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1DfIUTILuPs"
      },
      "source": [
        "# Taken from RL Adventure's [10] and OpenAI's Baselines [11] Repos.\n",
        "class SegmentTree(object):\n",
        "    def __init__(self, capacity, operation, neutral_element):\n",
        "        \"\"\"Build a Segment Tree data structure.\n",
        "        https://en.wikipedia.org/wiki/Segment_tree\n",
        "        Can be used as regular array, but with two\n",
        "        important differences:\n",
        "            a) setting item's value is slightly slower.\n",
        "               It is O(lg capacity) instead of O(1).\n",
        "            b) user has access to an efficient `reduce`\n",
        "               operation which reduces `operation` over\n",
        "               a contiguous subsequence of items in the\n",
        "               array.\n",
        "        Paramters\n",
        "        ---------\n",
        "        capacity: int\n",
        "            Total size of the array - must be a power of two.\n",
        "        operation: lambda obj, obj -> obj\n",
        "            and operation for combining elements (eg. sum, max)\n",
        "            must for a mathematical group together with the set of\n",
        "            possible values for array elements.\n",
        "        neutral_element: obj\n",
        "            neutral element for the operation above. eg. float('-inf')\n",
        "            for max and 0 for sum.\n",
        "        \"\"\"\n",
        "        assert capacity > 0 and capacity & (capacity - 1) == 0, \"capacity must be positive and a power of 2.\"\n",
        "        self._capacity = capacity\n",
        "        self._value = [neutral_element for _ in range(2 * capacity)]\n",
        "        self._operation = operation\n",
        "\n",
        "    def _reduce_helper(self, start, end, node, node_start, node_end):\n",
        "        if start == node_start and end == node_end:\n",
        "            return self._value[node]\n",
        "        mid = (node_start + node_end) // 2\n",
        "        if end <= mid:\n",
        "            return self._reduce_helper(start, end, 2 * node, node_start, mid)\n",
        "        else:\n",
        "            if mid + 1 <= start:\n",
        "                return self._reduce_helper(start, end, 2 * node + 1, mid + 1, node_end)\n",
        "            else:\n",
        "                return self._operation(\n",
        "                    self._reduce_helper(start, mid, 2 * node, node_start, mid),\n",
        "                    self._reduce_helper(mid + 1, end, 2 * node + 1, mid + 1, node_end)\n",
        "                )\n",
        "\n",
        "    def reduce(self, start=0, end=None):\n",
        "        \"\"\"Returns result of applying `self.operation`\n",
        "        to a contiguous subsequence of the array.\n",
        "            self.operation(arr[start], operation(arr[start+1], operation(... arr[end])))\n",
        "        Parameters\n",
        "        ----------\n",
        "        start: int\n",
        "            beginning of the subsequence\n",
        "        end: int\n",
        "            end of the subsequences\n",
        "        Returns\n",
        "        -------\n",
        "        reduced: obj\n",
        "            result of reducing self.operation over the specified range of array elements.\n",
        "        \"\"\"\n",
        "        if end is None:\n",
        "            end = self._capacity\n",
        "        if end < 0:\n",
        "            end += self._capacity\n",
        "        end -= 1\n",
        "        return self._reduce_helper(start, end, 1, 0, self._capacity - 1)\n",
        "\n",
        "    def __setitem__(self, idx, val):\n",
        "        # index of the leaf\n",
        "        idx += self._capacity\n",
        "        self._value[idx] = val\n",
        "        idx //= 2\n",
        "        while idx >= 1:\n",
        "            self._value[idx] = self._operation(\n",
        "                self._value[2 * idx],\n",
        "                self._value[2 * idx + 1]\n",
        "            )\n",
        "            idx //= 2\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        assert 0 <= idx < self._capacity\n",
        "        return self._value[self._capacity + idx]\n",
        "\n",
        "# Taken from RL Adventure's [10] and OpenAI's Baselines [11] Repos.\n",
        "class SumSegmentTree(SegmentTree):\n",
        "    def __init__(self, capacity):\n",
        "        super(SumSegmentTree, self).__init__(\n",
        "            capacity=capacity,\n",
        "            operation=operator.add,\n",
        "            neutral_element=0.0\n",
        "        )\n",
        "\n",
        "    def sum(self, start=0, end=None):\n",
        "        \"\"\"Returns arr[start] + ... + arr[end]\"\"\"\n",
        "        return super(SumSegmentTree, self).reduce(start, end)\n",
        "\n",
        "    def find_prefixsum_idx(self, prefixsum):\n",
        "        \"\"\"Find the highest index `i` in the array such that\n",
        "            sum(arr[0] + arr[1] + ... + arr[i - i]) <= prefixsum\n",
        "        if array values are probabilities, this function\n",
        "        allows to sample indexes according to the discrete\n",
        "        probability efficiently.\n",
        "        Parameters\n",
        "        ----------\n",
        "        perfixsum: float\n",
        "            upperbound on the sum of array prefix\n",
        "        Returns\n",
        "        -------\n",
        "        idx: int\n",
        "            highest index satisfying the prefixsum constraint\n",
        "        \"\"\"\n",
        "        assert 0 <= prefixsum <= self.sum() + 1e-5\n",
        "        idx = 1\n",
        "        while idx < self._capacity:  # while non-leaf\n",
        "            if self._value[2 * idx] > prefixsum:\n",
        "                idx = 2 * idx\n",
        "            else:\n",
        "                prefixsum -= self._value[2 * idx]\n",
        "                idx = 2 * idx + 1\n",
        "        return idx - self._capacity\n",
        "\n",
        "# Taken from RL Adventure's [10] and OpenAI's Baselines [11] Repos.\n",
        "class MinSegmentTree(SegmentTree):\n",
        "    def __init__(self, capacity):\n",
        "        super(MinSegmentTree, self).__init__(\n",
        "            capacity=capacity,\n",
        "            operation=min,\n",
        "            neutral_element=float('inf')\n",
        "        )\n",
        "\n",
        "    def min(self, start=0, end=None):\n",
        "        \"\"\"Returns min(arr[start], ...,  arr[end])\"\"\"\n",
        "\n",
        "        return super(MinSegmentTree, self).reduce(start, end)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsxsCF4sMM6N"
      },
      "source": [
        "## Prioritized Replay Buffer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3uKw2G2MNDV"
      },
      "source": [
        "# Edited from RL Adventure's [10] and OpenAI's Baselines [11] Repos.\n",
        "class ReplayBuffer(object):\n",
        "    def __init__(self, size):\n",
        "        \"\"\"Create Replay buffer.\n",
        "        Parameters\n",
        "        ----------\n",
        "        size: int\n",
        "            Max number of transitions to store in the buffer. When the buffer\n",
        "            overflows the old memories are dropped.\n",
        "        \"\"\"\n",
        "        self._storage = []\n",
        "        self._maxsize = size\n",
        "        self._next_idx = 0\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._storage)\n",
        "\n",
        "    def push(self, seq):\n",
        "        if self._next_idx >= len(self._storage):\n",
        "            self._storage.append(seq)\n",
        "        else:\n",
        "            self._storage[self._next_idx] = seq\n",
        "        self._next_idx = (self._next_idx + 1) % self._maxsize\n",
        "\n",
        "    def _encode_sample(self, idxes):\n",
        "        s_lst = torch.from_numpy(np.array([[self._storage[idx][0][time] for idx in idxes] for time in range(seq_len_with_burn_in+1)])).to(device).float()/255.0\n",
        "        a_lst = torch.tensor([[[self._storage[idx][1][time]] for idx in idxes] for time in range(seq_len_with_burn_in+1)]).to(device)\n",
        "        r_lst = torch.tensor([[[self._storage[idx][2][time]] for idx in idxes] for time in range(seq_len_with_burn_in+1)]).to(device)\n",
        "        done_mask_lst = torch.tensor([[[self._storage[idx][3][time]] for idx in idxes] for time in range(seq_len_with_burn_in+1)]).to(device)\n",
        "        h0, c0 = torch.cat([self._storage[idx][4][\"h0\"] for idx in idxes]).squeeze(1).unsqueeze(0).to(device), torch.cat([self._storage[idx][4][\"c0\"] for idx in idxes]).squeeze(1).unsqueeze(0).to(device)\n",
        "        return s_lst, a_lst, r_lst, done_mask_lst, {\"h0\": h0, \"c0\": c0}\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        \"\"\"Sample a batch of experiences.\n",
        "        Parameters\n",
        "        ----------\n",
        "        batch_size: int\n",
        "            How many transitions to sample.\n",
        "        Returns\n",
        "        -------\n",
        "        obs_batch: np.array\n",
        "            batch of observations\n",
        "        act_batch: np.array\n",
        "            batch of actions executed given obs_batch\n",
        "        rew_batch: np.array\n",
        "            rewards received as results of executing act_batch\n",
        "        next_obs_batch: np.array\n",
        "            next set of observations seen after executing act_batch\n",
        "        done_mask: np.array\n",
        "            done_mask[i] = 1 if executing act_batch[i] resulted in\n",
        "            the end of an episode and 0 otherwise.\n",
        "        \"\"\"\n",
        "        idxes = [random.randint(0, len(self._storage) - 1) for _ in range(batch_size)]\n",
        "        return self._encode_sample(idxes)\n",
        "\n",
        "# Edited from RL Adventure's [10] and OpenAI's Baselines [11] Repos.\n",
        "class PrioritizedReplayBuffer(ReplayBuffer):\n",
        "    def __init__(self, size, alpha):\n",
        "        \"\"\"Create Prioritized Replay buffer.\n",
        "        Parameters\n",
        "        ----------\n",
        "        size: int\n",
        "            Max number of transitions to store in the buffer. When the buffer\n",
        "            overflows the old memories are dropped.\n",
        "        alpha: float\n",
        "            how much prioritization is used\n",
        "            (0 - no prioritization, 1 - full prioritization)\n",
        "        See Also\n",
        "        --------\n",
        "        ReplayBuffer.__init__\n",
        "        \"\"\"\n",
        "        super(PrioritizedReplayBuffer, self).__init__(size)\n",
        "        assert alpha > 0\n",
        "        self._alpha = alpha\n",
        "\n",
        "        it_capacity = 1\n",
        "        while it_capacity < size:\n",
        "            it_capacity *= 2\n",
        "\n",
        "        self._it_sum = SumSegmentTree(it_capacity)\n",
        "        self._it_min = MinSegmentTree(it_capacity)\n",
        "        self._max_priority = 1.0\n",
        "\n",
        "    def push(self, *args, **kwargs):\n",
        "        \"\"\"See ReplayBuffer.store_effect\"\"\"\n",
        "        idx = self._next_idx\n",
        "        super(PrioritizedReplayBuffer, self).push(*args, **kwargs)\n",
        "        self._it_sum[idx] = self._max_priority ** self._alpha\n",
        "        self._it_min[idx] = self._max_priority ** self._alpha\n",
        "\n",
        "    def _sample_proportional(self, batch_size):\n",
        "        res = []\n",
        "        for _ in range(batch_size):\n",
        "            # TODO(szymon): should we ensure no repeats?\n",
        "            mass = random.random() * self._it_sum.sum(0, len(self._storage) - 1)\n",
        "            idx = self._it_sum.find_prefixsum_idx(mass)\n",
        "            res.append(idx)\n",
        "        return res\n",
        "\n",
        "    def sample(self, batch_size, beta=importance_sampling_exponent):\n",
        "        \"\"\"Sample a batch of experiences.\n",
        "        compared to ReplayBuffer.sample\n",
        "        it also returns importance weights and idxes\n",
        "        of sampled experiences.\n",
        "        Parameters\n",
        "        ----------\n",
        "        batch_size: int\n",
        "            How many transitions to sample.\n",
        "        beta: float\n",
        "            To what degree to use importance weights\n",
        "            (0 - no corrections, 1 - full correction)\n",
        "        Returns\n",
        "        -------\n",
        "        obs_batch: np.array\n",
        "            batch of observations\n",
        "        act_batch: np.array\n",
        "            batch of actions executed given obs_batch\n",
        "        rew_batch: np.array\n",
        "            rewards received as results of executing act_batch\n",
        "        next_obs_batch: np.array\n",
        "            next set of observations seen after executing act_batch\n",
        "        done_mask: np.array\n",
        "            done_mask[i] = 1 if executing act_batch[i] resulted in\n",
        "            the end of an episode and 0 otherwise.\n",
        "        weights: np.array\n",
        "            Array of shape (batch_size,) and dtype np.float32\n",
        "            denoting importance weight of each sampled transition\n",
        "        idxes: np.array\n",
        "            Array of shape (batch_size,) and dtype np.int32\n",
        "            idexes in buffer of sampled experiences\n",
        "        \"\"\"\n",
        "        assert beta > 0\n",
        "\n",
        "        idxes = self._sample_proportional(batch_size)\n",
        "\n",
        "        weights = []\n",
        "        p_min = self._it_min.min() / self._it_sum.sum()\n",
        "        max_weight = (p_min * len(self._storage)) ** (-beta)\n",
        "\n",
        "        for idx in idxes:\n",
        "            p_sample = self._it_sum[idx] / self._it_sum.sum()\n",
        "            weight = (p_sample * len(self._storage)) ** (-beta)\n",
        "            weights.append(weight / max_weight)\n",
        "        weights = torch.tensor(weights, device=device)\n",
        "        encoded_sample = self._encode_sample(idxes)\n",
        "        return encoded_sample, [weights, idxes]\n",
        "\n",
        "    def update_priorities(self, idxes, priorities):\n",
        "        \"\"\"Update priorities of sampled transitions.\n",
        "        sets priority of transition at index idxes[i] in buffer\n",
        "        to priorities[i].\n",
        "        Parameters\n",
        "        ----------\n",
        "        idxes: [int]\n",
        "            List of idxes of sampled transitions\n",
        "        priorities: [float]\n",
        "            List of updated priorities corresponding to\n",
        "            transitions at the sampled idxes denoted by\n",
        "            variable `idxes`.\n",
        "        \"\"\"\n",
        "        assert len(idxes) == len(priorities)\n",
        "        for idx, priority in zip(idxes, priorities):\n",
        "            assert priority > 0\n",
        "            assert 0 <= idx < len(self._storage)\n",
        "            self._it_sum[idx] = priority ** self._alpha\n",
        "            self._it_min[idx] = priority ** self._alpha\n",
        "\n",
        "            self._max_priority = max(self._max_priority, priority)\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-dIWII4Xnxd"
      },
      "source": [
        "## Setup Prioritized Replay Buffer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_DDpPASXhBP"
      },
      "source": [
        "memory = PrioritizedReplayBuffer(buffer_limit, priority_exponent)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_d3qlENYB-w"
      },
      "source": [
        "## Duelling LSTM Q-Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Cv-fvvuYE8X"
      },
      "source": [
        "# Based off Facebook's RL Assembly [9] Repo.\n",
        "class DuellingLSTMNet(nn.Module):\n",
        "    def __init__(self, device, num_action):\n",
        "        super().__init__()\n",
        "\n",
        "        # Parameters\n",
        "        self.frame_stack = 4\n",
        "        self.conv_out_dim = 3136\n",
        "        self.hid_dim = 512\n",
        "        self.num_lstm_layer = 1\n",
        "        self.num_action = num_action\n",
        "\n",
        "        # Convolutional state encoder.\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(self.frame_stack, 32, 8, stride=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, 3, stride=1),\n",
        "            nn.ReLU(),\n",
        "        ).to(device)\n",
        "\n",
        "        # LSTM to take in encoded states from CNN (and the hidden state at beginning of sequence) and output a state representation with longer term dependencies and information.\n",
        "        self.lstm = nn.LSTM(\n",
        "            self.conv_out_dim, self.hid_dim, num_layers=self.num_lstm_layer\n",
        "        ).to(device)\n",
        "\n",
        "        # If passing actions and rewards too (instead) like R2D2 does.\n",
        "        # self.lstm = nn.LSTM(\n",
        "        #     self.conv_out_dim + num_action + 1, self.hid_dim, num_layers=self.num_lstm_layer\n",
        "        # ).to(device)\n",
        "\n",
        "        # Value function fully connected layers: {\n",
        "            \n",
        "          # This design is inspired by the findings of the D2RL paper, which allows our network to become much deeper than usual.\n",
        "        # Directly connected to the LSTM output.\n",
        "        self.fc_v_1 = nn.Sequential(\n",
        "            nn.Linear(self.hid_dim, self.hid_dim),\n",
        "            nn.ReLU()\n",
        "        ).to(device)\n",
        "        \n",
        "        # Contains a skip connection to the CNN output.\n",
        "        self.fc_v_2 = nn.Sequential(\n",
        "             nn.Linear(self.hid_dim + self.conv_out_dim, self.hid_dim),\n",
        "             nn.ReLU()\n",
        "        ).to(device)\n",
        "\n",
        "        # Contains a skip connection to the LSTM output.\n",
        "        self.fc_v_3 = nn.Sequential(\n",
        "            nn.Linear(self.hid_dim + self.hid_dim, self.hid_dim),\n",
        "            nn.ReLU()\n",
        "        ).to(device)\n",
        "        \n",
        "        # Contains a skip connection to the LSTM output.\n",
        "        self.fc_v_4 = nn.Linear(self.hid_dim + self.hid_dim, 1).to(device)\n",
        "\n",
        "        # }\n",
        "\n",
        "        # Advantage function fully connected layers: {\n",
        "            \n",
        "          # This design is inspired by the findings of the D2RL paper, which allows our network to become much deeper than usual.\n",
        "        # Directly connected to the LSTM output.\n",
        "        self.fc_a_1 = nn.Sequential(\n",
        "            nn.Linear(self.hid_dim, self.hid_dim),\n",
        "            nn.ReLU()\n",
        "        ).to(device)\n",
        "\n",
        "        # Contains a skip connection to the CNN output.\n",
        "        self.fc_a_2 = nn.Sequential(\n",
        "            nn.Linear(self.hid_dim + self.conv_out_dim, self.hid_dim),\n",
        "            nn.ReLU()\n",
        "        ).to(device)\n",
        "        # Contains a skip connection to the LSTM output.\n",
        "        self.fc_a_3 = nn.Sequential(\n",
        "            nn.Linear(self.hid_dim + self.hid_dim, self.hid_dim),\n",
        "            nn.ReLU()\n",
        "        ).to(device)\n",
        "\n",
        "        # Contains a skip connection to the LSTM output.\n",
        "        self.fc_a_4 = nn.Linear(self.hid_dim + self.hid_dim, self.num_action).to(device)\n",
        "\n",
        "        # }\n",
        "\n",
        "        self.lstm.flatten_parameters()\n",
        "    \n",
        "    def get_h0(self, batchsize):\n",
        "        \"\"\"\n",
        "        Retrieve initial hidden state of LSTM.\n",
        "        \"\"\"\n",
        "        shape = (self.num_lstm_layer, batchsize, self.hid_dim)\n",
        "        hid = {\"h0\": torch.zeros(*shape, device=device), \"c0\": torch.zeros(*shape, device=device)}\n",
        "        return hid\n",
        "\n",
        "    def duel(self, v, a):\n",
        "        \"\"\"\n",
        "        Takes in Q-value outputs from Value and Advantage networks, and produces the Duelling Q-Networks outputs.\n",
        "        \"\"\"\n",
        "        q = v + a - a.mean(2, keepdim=True)\n",
        "        return q\n",
        "\n",
        "    def _conv_forward(self, s):\n",
        "        \"\"\"\n",
        "        Send observation through CNN.\n",
        "        \"\"\"\n",
        "        assert s.dim() == 4  # [batch, c, h, w]\n",
        "        x = self.net(s) #state to representaion\n",
        "        x = x.view(s.size(0), self.conv_out_dim)\n",
        "        return x\n",
        "\n",
        "    def advantage(self, o, s):\n",
        "        \"\"\"\n",
        "        Retrieve Q-values for each action from the current state.\n",
        "        Taken from the D2RL [2] paper's insights and architecture.\n",
        "        o: LSTM output\n",
        "        s: CNN output\n",
        "        \"\"\"\n",
        "        a = self.fc_a_1(o)\n",
        "        a = torch.cat([a, s], dim=2)\n",
        "        a = self.fc_a_2(a)\n",
        "        a = torch.cat([a, o], dim=2)\n",
        "        a = self.fc_a_3(a)\n",
        "        a = torch.cat([a, o], dim=2)\n",
        "        a = self.fc_a_4(a)\n",
        "        return a\n",
        "\n",
        "    def value(self, o, s):\n",
        "        \"\"\"\n",
        "        Retrieve Q-value for the value of the current state.\n",
        "        Taken from the D2RL [2] paper's insights and architecture.\n",
        "        o: LSTM output\n",
        "        s: CNN output\n",
        "        \"\"\"\n",
        "        v = self.fc_v_1(o)\n",
        "        v = torch.cat([v, s], dim=2)\n",
        "        v = self.fc_v_2(v)\n",
        "        v = torch.cat([v, o], dim=2)\n",
        "        v = self.fc_v_3(v)\n",
        "        v = torch.cat([v, o], dim=2)\n",
        "        v = self.fc_v_4(v)\n",
        "        return v\n",
        "\n",
        "    def act(self, obs, a, r, hid, epsilon_greedy=True):\n",
        "        \"\"\"\n",
        "        Retrieve the action an agent(s) should take according to the Duelling Network (in this case simply the Advantage Network), with epsilon greedy policy support.\n",
        "        Simultaneously return the next hidden state of the LSTM.\n",
        "        \"\"\"\n",
        "        x = self._conv_forward(obs)\n",
        "        # x: [batch, hid]\n",
        "        x = x.unsqueeze(0)\n",
        "        # x: [1, batch, hid]\n",
        "\n",
        "        # If passing actions and rewards too (instead) like R2D2 does.\n",
        "        # x = torch.cat([x,a,r], axis = 2)\n",
        "\n",
        "        o, (h, c) = self.lstm(x, (hid[\"h0\"], hid[\"c0\"]))\n",
        "        if epsilon_greedy and random.random() < epsilon:\n",
        "          greedy_action = random.randrange(self.num_action)\n",
        "        else:\n",
        "          a = self.advantage(o, x)\n",
        "          a = a.squeeze(0)\n",
        "          # a: [batch, num_action]\n",
        "          legal_a = (1 + a - a.min())\n",
        "          greedy_action = legal_a.argmax(1).detach()\n",
        "        \n",
        "          # If instead using this network to process batches of actions (in a distributed setting for example):\n",
        "          # if epsilon_greedy:\n",
        "            # random_actions = torch.randint(low=0, high=self.num_action, size=(batch_size,))\n",
        "            # probs = torch.rand(batch_size)\n",
        "            # greedy_action = torch.where(probs < epsilon, random_actions, greedy_action)\n",
        "\n",
        "        return int(greedy_action), {\"h0\": h.detach(), \"c0\": c.detach()}\n",
        "\n",
        "    def unroll_rnn(self, obs, a, r, hid):\n",
        "        \"\"\"\n",
        "        Send observation through both CNN and LSTM.\n",
        "        \"\"\"\n",
        "        s = obs\n",
        "        assert s.dim() == 5  # [seq, batch, c, h, w]\n",
        "        seq, batch, c, h, w = s.size()\n",
        "        s = s.view(seq * batch, c, h, w)\n",
        "        # Send through CNN.\n",
        "        x = self.net(s)\n",
        "        x = x.view(seq, batch, self.conv_out_dim)\n",
        "\n",
        "        # If passing actions and rewards too (instead) like R2D2 does.\n",
        "        # x = torch.cat([x,a,r], axis = 2)\n",
        "\n",
        "        o, (h, c) = self.lstm(x, (hid[\"h0\"], hid[\"c0\"]))\n",
        "\n",
        "        # o: LSTM output, x: CNN output.\n",
        "        return o, x, {\"h0\": h, \"c0\": c}\n",
        "\n",
        "    def forward(self, obs, a, r, hid):\n",
        "        \"\"\"\n",
        "        Send observation through the network and return Q-values.\n",
        "        return:\n",
        "            q(s, a): [seq, batch, num_action]\n",
        "            hid(s_n): [batch] (n=seq, final hidden state, used for LSTM burnin)\n",
        "        \"\"\"\n",
        "        o, x, hid = self.unroll_rnn(obs, a, r, hid)\n",
        "        # o: LSTM output, x: CNN output.\n",
        "        a = self.advantage(o, x)\n",
        "        v = self.value(o, x)\n",
        "        q = self.duel(v, a)\n",
        "        return q, hid"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrRJaV7-jOv4"
      },
      "source": [
        "## Q-Networks Setup for Double Q Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJ4arAsjbKtk"
      },
      "source": [
        "q = DuellingLSTMNet(device, num_actions)\n",
        "q_target = DuellingLSTMNet(device, num_actions)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCgBZd3clUbh"
      },
      "source": [
        "## Setup Optimizer for Online Network Backpropagation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IygL6IJzlZYp"
      },
      "source": [
        "optimizer = optim.Adam(q.parameters(), lr=learning_rate, eps=adam_eps)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bjyr3Tmqg22"
      },
      "source": [
        "## Recommence Training from Checkpoint and Sync Networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qttq9-IqXjH"
      },
      "source": [
        "start_ep = 0\n",
        "if restart:\n",
        "    params = torch.load('training/save{}.chkpt'.format(tag))\n",
        "    q.load_state_dict(params['q'])\n",
        "    optimizer.load_state_dict(params[\"optimizer\"])\n",
        "    start_ep = params[\"n_episode\"]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bK7wzgqTrQ-i"
      },
      "source": [
        "## Sync Online and Target Q-Networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZ-BeDr1rMdf",
        "outputId": "9bd8b211-df31-43da-aecd-7d7501e771c0"
      },
      "source": [
        "q_target.load_state_dict(q.state_dict())"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zm45yV1mrui5"
      },
      "source": [
        "## Training Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8ietWqIr7Zr"
      },
      "source": [
        "# Edited from Google's SEED RL [8] Repo.\n",
        "def value_function_rescaling(x):\n",
        "  \"\"\"Value function rescaling used in R2D2 paper [1], see table 2, or Proposition A.2 in paper \"Observe and Look Further\" [7].\"\"\"\n",
        "  return torch.sign(x) * (torch.sqrt(torch.abs(x) + 1.) - 1.) + vfEpsilon * x\n",
        "\n",
        "# Edited from Google's SEED RL [8] Repo.\n",
        "def inverse_value_function_rescaling(x):\n",
        "  \"\"\"Inverse of the above function. See Proposition A.2 in paper \"Observe and Look Further\" [7].\"\"\"\n",
        "  return torch.sign(x) * (\n",
        "      torch.square(((torch.sqrt(\n",
        "          1. + 4. * vfEpsilon * (torch.abs(x) + 1. + vfEpsilon))) - 1.) / (2. * vfEpsilon)) -\n",
        "      1.)\n",
        "  \n",
        "# Edited from Google's SEED RL [8] Repo.\n",
        "def n_step_bellman_target(rewards, done_mask, q_target, gamma, n_steps):\n",
        "  r\"\"\"Computes n-step Bellman targets.\n",
        "  See section 2.3 of R2D2 [1] paper (which does not mention the logic around end of\n",
        "  episode).\n",
        "  Args:\n",
        "    rewards: <float32>[time, batch_size] tensor. This is r_t in the equations\n",
        "      below.\n",
        "    done_mask: <bool>[time, batch_size] tensor. This is done_mask_t in the equations\n",
        "      below. done_mask_t should be false if the episode is done just after\n",
        "      experimenting reward r_t.\n",
        "    q_target: <float32>[time, batch_size] tensor. This is Q_target(s_{t+1}, a*)\n",
        "      (where a* is an action chosen by the caller).\n",
        "    gamma: Exponential RL discounting.\n",
        "    n_steps: The number of steps to look ahead for computing the Bellman\n",
        "      targets.\n",
        "  Returns:\n",
        "    y_t targets as <float32>[time, batch_size] tensor.\n",
        "    When n_steps=1, this is just:\n",
        "    $$r_t + gamma * (1 - done_t) * Q_{target}(s_{t+1}, a^*)$$\n",
        "    In the general case, this is:\n",
        "    $$(\\sum_{i=0}^{n-1} \\gamma ^ {i} * notdone_{t, i-1} * r_{t + i}) +\n",
        "      \\gamma ^ n * notdone_{t, n-1} * Q_{target}(s_{t + n}, a^*) $$\n",
        "    where notdone_{t,i} is defined as:\n",
        "    $$notdone_{t,i} = \\prod_{k=h0}^{k=i}(1 - done_mask_{t+k})$$\n",
        "    The last n_step-1 targets cannot be computed with n_step returns, since we\n",
        "    run out of Q_{target}(s_{t+n}). Instead, they will use n_steps-1, .., 1 step\n",
        "    returns. For those last targets, the last Q_{target}(s_{t}, a^*) is re-used\n",
        "    multiple times.\n",
        "    However, in this implementation the last n_steps-1 are truncated in the\n",
        "    training function and are not used.\n",
        "    \n",
        "  \"\"\"\n",
        "  # We append n_steps - 1 times the last q_target. They are divided by gamma **\n",
        "  # k to correct for the fact that they are at a 'fake' indice, and will\n",
        "  # therefore end up being multiplied back by gamma ** k in the loop below.\n",
        "  # We prepend 0s that will be discarded at the first iteration below.\n",
        "  bellman_target = torch.cat(\n",
        "      [torch.zeros_like(q_target[0:1]), q_target] +\n",
        "      [q_target[-1:] / gamma ** k\n",
        "       for k in range(1, n_steps)],\n",
        "      axis=0)\n",
        "  # Pad with n_steps 0s. They will be used to compute the last n_steps-1\n",
        "  # targets (having 0 values is important).\n",
        "  done_mask = torch.cat([done_mask] + [torch.ones_like(done_mask[0:1])] * n_steps, axis=0)\n",
        "  rewards = torch.cat([rewards] + [torch.zeros_like(rewards[0:1])] * n_steps,\n",
        "                      axis=0)\n",
        "  # Iteratively build the n_steps targets. After the i-th iteration (1-based),\n",
        "  # bellman_target is effectively the i-step returns.\n",
        "  for _ in range(n_steps):\n",
        "    rewards = rewards[:-1]\n",
        "    done_mask = done_mask[:-1]\n",
        "    bellman_target = (\n",
        "        rewards + gamma * done_mask * bellman_target[1:])\n",
        "\n",
        "  return bellman_target"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jj2SZihhsbHe"
      },
      "source": [
        "## Training Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-UX0tcMsg6r"
      },
      "source": [
        "def train(q, q_target, memory, optimizer):\n",
        "    # If you want to train multiple iterations per episode\n",
        "    # for _ in range(5):\n",
        "        (s,a,r,done_mask,hids), (weights, idxes)  = memory.sample(batch_size)\n",
        "        \n",
        "        # WARNING: \n",
        "        # s also has last s_prime attached!\n",
        "        # done_mask has 1.0 prepended once!\n",
        "\n",
        "        # one_hot_a and r_scaled are only used if passing actions and rewards too (instead) like R2D2 does.\n",
        "        one_hot_a = torch.nn.functional.one_hot(a, num_classes=num_actions).squeeze()\n",
        "        r_scaled = r/250.0\n",
        "\n",
        "        # Handle LSTM burnin.\n",
        "        if l_burnin:\n",
        "          with torch.no_grad():\n",
        "              _, online_hid = q(s[:l_burnin],one_hot_a[:l_burnin], r_scaled[:l_burnin], hids)\n",
        "              _, target_hid = q_target(s[:l_burnin],one_hot_a[:l_burnin], r_scaled[:l_burnin], hids)\n",
        "        else:\n",
        "          online_hid = hids \n",
        "          target_hid = hids\n",
        "        s,one_hot_a,r,r_scaled, done_mask, a = s[l_burnin:], one_hot_a[l_burnin:], r[l_burnin:],r_scaled[l_burnin:],  done_mask[l_burnin:], a[l_burnin:]\n",
        "\n",
        "        # Retrieve online network's Q-values.\n",
        "        q_out, _ = q(s,one_hot_a,r_scaled, online_hid)\n",
        "\n",
        "        # Get Q-value for maximum action.\n",
        "        q_a = q_out[:-1].gather(2,a[1:])*done_mask[:-1] #replay_q\n",
        "        # Truncate last n_steps-1 Q-values where the n-step Bellman targets are \"incomplete\".\n",
        "        q_a = q_a[:-n_steps+1]\n",
        "\n",
        "        # Compute n-step Bellman targets.\n",
        "        with torch.no_grad():\n",
        "          # Double Q-Learning {\n",
        "          # This means using argmax from online Q-network, but corresponding Q-values from target network to increase stability\n",
        "          greedy_action = q_out[1:].max(2)[1].unsqueeze(2)\n",
        "          target_q_prime, _ = q_target(s,one_hot_a,r_scaled, target_hid)\n",
        "          target_q_prime = target_q_prime[1:]\n",
        "          max_q_prime = target_q_prime.gather(2, greedy_action) * done_mask[1:]\n",
        "          # }\n",
        "\n",
        "          max_q_prime = inverse_value_function_rescaling(max_q_prime)\n",
        "          \n",
        "          target = n_step_bellman_target(r[1:], done_mask[1:], max_q_prime, gamma, n_steps)\n",
        "          \n",
        "          target = value_function_rescaling(target)[:-n_steps+1]\n",
        "\n",
        "        # Calculate TD errors and batch-wise weighted loss.\n",
        "        abs_td_errors = torch.abs(q_a - target.detach()).float()\n",
        "        loss = 0.5 * abs_td_errors.square().sum(0)\n",
        "        loss= (loss * weights).mean()\n",
        "\n",
        "        # Perform an optimizer step using clipped gradients, as per Dueling Network Architectures for Deep Reinforcement Learning.\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(q.parameters(), 10)\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update Prioritized Replay Buffer Priorities.\n",
        "        with torch.no_grad():\n",
        "          priorities = torch.max(abs_td_errors, axis=0).values * eta + torch.mean(abs_td_errors, axis=0) * (1 - eta)\n",
        "          memory.update_priorities(idxes, priorities)\n",
        "\n",
        "        # Clean up memory\n",
        "        del s,a,r,done_mask,hids, q_out, q_a, greedy_action, max_q_prime, target, abs_td_errors, priorities, loss, weights, idxes"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avyG0oMc9YKH"
      },
      "source": [
        "## Setup Temporary and Helper Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7TMuRu39jRc"
      },
      "source": [
        "# Defined for memory efficiency of the Replay Buffer.\n",
        "zeroFloatList = [0.0]\n",
        "zeroIntList   = [0]\n",
        "\n",
        "# Variables for statistic printing.\n",
        "score         = 0.0\n",
        "marking       = []"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2_NpHjg9In1"
      },
      "source": [
        "## Acting and Learning Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQQzZZuZ9OqL",
        "outputId": "027b4176-b6cf-4b48-f8ff-ecbfc91f05ac"
      },
      "source": [
        "for n_episode in range(start_ep, int(1e32)):\n",
        "# 3 stage linear epsilon annealing:\n",
        "    if n_episode < 200000:\n",
        "        # Linear annealing from 50% to 2%, then constant from episode 100k -> 200k.\n",
        "        epsilon = max(0.02, 0.50 - 0.01*(n_episode/2000), 0) \n",
        "    else:\n",
        "        # Linear annealing from 2% to 0% from episode 200k\n",
        "        epsilon = max(0,  0.02 - 0.02*(n_episode-200000)/200000)\n",
        "    \n",
        "    s = env.reset()\n",
        "\n",
        "    # Perform 1 random action at beginning of episode.\n",
        "    a = random.randrange(num_actions)\n",
        "    s, r, done, _ = env.step(a)\n",
        "    if done:\n",
        "        continue\n",
        "    s_list,a_list,r_list, done_mask_list = [], [a], [r], [1.0]\n",
        "   \n",
        "    # Get LSTM initial hidden state.\n",
        "    seq1_initial_episode_hid = q.get_h0(1)\n",
        "    hid = seq1_initial_episode_hid\n",
        "\n",
        "    # Reset variables.\n",
        "    done = False\n",
        "    score = 0.\n",
        "    current_seq1_len = 0\n",
        "    notFirstSeq = False\n",
        "\n",
        "    with torch.no_grad():\n",
        "        while True:\n",
        "            current_seq1_len += 1\n",
        "            \n",
        "            # Retrieve action to take from online Q-network.\n",
        "            a, hid = q.act(torch.from_numpy(np.array(s)).to(device).unsqueeze(0).float()/255.0, torch.nn.functional.one_hot(torch.tensor([a]).view(1,1), num_classes=num_actions).to(device), torch.tensor([r]).view(1,1,1).to(device)/250.0, hid)\n",
        "            \n",
        "            # Step environment.\n",
        "            s_prime, r, done, _ = env.step(a)\n",
        "            \n",
        "            done_mask = 0.0 if done else 1.0\n",
        "            s_list.append(s)\n",
        "            a_list.append(a)\n",
        "            r_list.append(r)\n",
        "            done_mask_list.append(done_mask)\n",
        "\n",
        "            score += r\n",
        "\n",
        "            if done:\n",
        "              deltaEndSeq0 = seq_len_with_burn_in_minus_overlap - current_seq1_len\n",
        "              if notFirstSeq:\n",
        "                # Pad end of episode with zeros and dones and build seq0.\n",
        "                s0_list = s_list + [s_prime] * (deltaEndSeq0+1)\n",
        "                a0_list = a_list + zeroIntList * deltaEndSeq0\n",
        "                r0_list = r_list + zeroFloatList * deltaEndSeq0\n",
        "                done0_mask_list = done_mask_list + zeroFloatList * deltaEndSeq0\n",
        "                \n",
        "                # NOTE: seq0 is always long enough if this is not the first sequence (chunk) of the episode.\n",
        "                # Submit longer sequence seq0 into buffer.\n",
        "                memory.push((s0_list,a0_list,r0_list, done0_mask_list, seq0_initial_episode_hid))\n",
        "                \n",
        "                # If seq1 is long enough.\n",
        "                if current_seq1_len > l_burnin + minimumLen:\n",
        "                  # Pad end of episode with zeros and dones and build seq1.\n",
        "                  s1_list = s0_list[overlap:] + [s0_list[-2]] * overlap\n",
        "                  a1_list = a0_list[overlap:] + zeroIntList * overlap\n",
        "                  r1_list = r0_list[overlap:] + zeroFloatList * overlap\n",
        "                  done1_mask_list = done0_mask_list[overlap:] + zeroFloatList * overlap\n",
        "\n",
        "                  # Submit shorter sequence seq1 into buffer.\n",
        "                  memory.push((s1_list,a1_list,r1_list, done1_mask_list, seq1_initial_episode_hid))\n",
        "              \n",
        "              # If this is the first sequence (chunk) of the episode.\n",
        "              elif current_seq1_len > l_burnin + minimumLen:\n",
        "                deltaEndSeq1 = deltaEndSeq0 + overlap\n",
        "                # Pad end of episode with zeros and dones and build seq1.\n",
        "                s1_list = s_list + [s_prime] * (deltaEndSeq1+1)\n",
        "                a1_list = a_list + zeroIntList * deltaEndSeq1\n",
        "                r1_list = r_list + zeroFloatList * deltaEndSeq1\n",
        "                done1_mask_list = done_mask_list + zeroFloatList * deltaEndSeq1\n",
        "\n",
        "                # Submit the only sequence into buffer.\n",
        "                memory.push((s1_list,a1_list,r1_list, done1_mask_list, seq1_initial_episode_hid))\n",
        "              break\n",
        "            \n",
        "            # When longer of 2 sequences (seq0) reaches correct length, submit to buffer and handle sequence switching.\n",
        "            if seq_len_with_burn_in_minus_overlap == current_seq1_len: # If seq0len = seq_len_with_burn_in : handle switch over\n",
        "              # Unless seq0 does not yet exist.\n",
        "              if notFirstSeq:\n",
        "                memory.push((s_list[:seq_len_with_burn_in] + [s_prime],a_list[:seq_len_with_burn_in+1],r_list[:seq_len_with_burn_in+1], done_mask_list[:seq_len_with_burn_in+1], seq0_initial_episode_hid))\n",
        "                s_list,a_list,r_list, done_mask_list = s_list[overlap:], a_list[overlap:], r_list[overlap:],  done_mask_list[overlap:]\n",
        "\n",
        "              current_seq1_len -= overlap\n",
        "              notFirstSeq = True\n",
        "              seq0_initial_episode_hid = seq1_initial_episode_hid\n",
        "              seq1_initial_episode_hid = hid\n",
        "\n",
        "            # Once there is nothing left to do set current state as the previous next state ready for next iteration.\n",
        "            s = s_prime\n",
        "\n",
        "    # If the buffer is big enough, begin training at the end of each new episode.\n",
        "    if len(memory)>4000:  \n",
        "        train(q, q_target, memory, optimizer)\n",
        "\n",
        "    marking.append(score)\n",
        "\n",
        "    # Sync target and online Q-networks periodically.\n",
        "    if n_episode%sync_target_every==0 and n_episode!=0:\n",
        "        q_target.load_state_dict(q.state_dict())\n",
        "\n",
        "    # Save model checkpoint periodically.\n",
        "    if n_episode%10000 == 0:\n",
        "        torch.save({'q':q.state_dict(), 'optimizer':optimizer.state_dict(), 'n_episode':n_episode}, 'training/save{0}.chkpt'.format(tag))\n",
        "    \n",
        "    # Handle statistics printing and empty cache.\n",
        "    if n_episode%100 == 0:\n",
        "        print(\"marking, episode: {}, score: {:.1f}, mean_score: {:.2f}, std_score: {:.2f}\".format(\n",
        "            n_episode, score, np.array(marking).mean(), np.array(marking).std()))\n",
        "        \n",
        "        # TensorBoard integration.\n",
        "        writer.add_scalar('score',\n",
        "                           np.array(marking).mean(),\n",
        "                            n_episode)\n",
        "        writer.add_scalar(\"eps\", epsilon, n_episode)\n",
        "        \n",
        "        marking = []\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    if n_episode%print_every==0 and n_episode!=0:\n",
        "        print(\"episode: {}, score: {:.1f}, epsilon: {:.2f}\".format(n_episode, score, epsilon))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "marking, episode: 0, score: 0.0, mean_score: 0.00, std_score: 0.00\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}